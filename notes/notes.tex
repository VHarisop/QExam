\documentclass[a4paper]{article}

\usepackage[margin=1in]{geometry}
\usepackage{../latex-macros/macros/latex-macros}
\usepackage{subcaption}

% Several math packages to use
\usepackage{amsmath, amssymb, amsthm}
\usepackage{bm, cancel}

\usepackage[boxruled]{algorithm2e}
\usepackage{caption}
\newenvironment{code}{\captionsetup{type=listing}}{}

% Include pgfplots / tikz options here
% PStricks package
\usepackage{pstricks}
\usepackage{pstricks-add}
\usepackage{tikz}
\usepackage{pgfplots}
\usetikzlibrary{
    matrix,chains,positioning,decorations.pathreplacing,arrows,patterns}
\pgfplotsset{compat=newest, clip bounding box=default tikz}
\usepgfplotslibrary{fillbetween}

\usepackage[capitalize]{cleveref}
% Allow floats within equations to split
\allowdisplaybreaks
\setlength\parindent{0pt}

\usepackage[square,numbers,sort]{natbib}
\usepackage{svg}

\newcommand{\xstar}{x^{\star}}

\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}

\begin{document}

% Title here
\title{\textsc{
	Notes for \textit{Randomized Sketches of Convex Programs with Sharp
	Guarantees}}}
\author{
    \textsc{Vasileios Charisopoulos}\thanks{\quad
        School of Operations Research and Information Engineering, Cornell University,
        Ithaca, NY 14850, USA.
   \email{vc333@cornell.edu}}}
\date{\today}
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
This is a set of notes covering Pilanci \& Wainwright's \textit{Randomized
Sketches of Convex Programs with Sharp Guarantess}~\cite{PilWain15}. Emphasis
is given on the proof techniques, with additional explanations given for
several proofs.
\end{abstract}

\begin{lemma}[Sudakov-Fernique comparison inequality~\cite{Versh18}]
    \label{lemma:sudakov_fernique}
    Let $\set{X_{t}}_{t \in T}, \set{Y_t}_{t \in T}$ be two gaussian processes
    satisfying $\expec{X_t} = \expec{Y_t} = 0, \; \forall t \in T$. If
    \[
        \expec{X_t^2} \leq \expec{Y_t^2},
    \]
    then it holds that
    \[
        \expec{\sup_{t \in T} X_t} \leq \expec{\sup_{t \in T} Y_t}.
    \]
\end{lemma}

\begin{proposition}[Corollary of Theorem D in~\cite{MenPajTom07}]
    Let $\set{s_i}_{i=1}^m$ be i.i.d. centered $\sigma$-subgaussian random
    variables with unit covariance. Then $\exists c_1, c_2 > 0$ universal
    constants such that, $\forall \cY \subseteq \Sbb^{n - 1}$, it holds that:
    \[
        \prob{\sup_{y \in \cY} \abs{y^\top \left(
            \frac{1}{m} S^\top S - I_n
        \right) y} \leq c_1 \frac{\mathbb{W}(\cY)}{\sqrt{m}} + \delta
        } \geq 1 - e^{-\frac{c_2 m \delta^2}{\sigma^4}}.
    \]
\end{proposition}
\begin{proof}
    Set $F = \set{\ip{v, \cdot} \mmid v \in A \cK \cap \Sbb^{n-1}}$ and appeal
    to~\cite[Theorem D]{MenPajTom07}, to obtain that
    \[
        \sup_{y \in \cY} \abs{\frac{1}{m} \sum_{i=1}^m
        v^\top s_i s_i^\top v - \expec{v^\top s_i s_i^\top v}}
        \leq \theta,
    \]
    for any $\theta$ satisfying $\theta \geq \frac{c_1}{\sqrt{m}} \alpha
    \gamma_2(F, \norm{\cdot}_{\psi_2})$, where $\gamma_2$ is Talagrand's
    $\gamma_2$-functional. Since $s_i$ are isotropic and subgaussian, the
    following holds~\cite[Section 2]{MenPajTom07}:
    \[
        \gamma_2(F, \norm{\cdot}) \leq
        \beta \mathbb{W}(A \cK \cap \Sbb^{n - 1})
    \]
    Therefore, setting $\theta = \frac{c_1}{\sqrt{m}} \alpha \beta
    \mathbb{W}(A \cK \cap \Sbb^{n-1})$ and relabelling, gives us the result
    in the desired form.
\end{proof}

\paragraph{Details for Corollary 2}
Consider $r := \rank(A)$. In that case, any element $v \in \mathrm{Im}(A)$ can
be written as
\[
    v = \sum_{i=1}^{r} \lambda_i a_i, \; \lambda_i \in \Rbb,
    \; a_i^\top a_j = \begin{cases}
        1, & i = j \\
        0, & i \neq j
    \end{cases}.
\]
In other words, $\set{a_i}_{i=1}^r$ is an orthonormal basis of $A$. We can thus
write
\begin{align*}
    \sup_{u \in \Rbb^d} \frac{\abs{\ip{Au, g}}}{\norm{Au}_2} &=
    \sup_{\bm{\lambda}} \frac{\abs{\sum_{i=1}^r \lambda_i a_i^\top g}}{
        \norm{\sum_{i=1}^r \lambda_i a_i}_2} \\
    &= \sup_{\lambda}
        \abs{\sum_{i=1}^r \frac{\lambda_i}{\norm{\lambda}_2} a_i^\top g} \\
    &= \sup_{\eta \in \Sbb^{\rank(A) - 1}} \abs{\eta^\top \tilde{g}}
\end{align*}
Notice that $a_i^\top g \perp a_j^\top g$, since orthogonal transformations of
Gaussian variables lead to independent gaussian variables. Therefore,
$\tilde{g} \sim \cN(0, I_r)$ itself. From the above:
\begin{align*}
    \expec{\sup_{u \in \Rbb^d} \frac{\abs{\ip{Au, g}}}{\norm{Au}_2}} &=
    \expec{\sup_{\eta \in \Sbb^{\rank(A) - 1}} \abs{\eta^\top \tilde{g}}} \leq
    \expec{\norm{\tilde{g}}_2} =
    \sqrt{\rank(A)}.
\end{align*}

\paragraph{Details for Lemma 1}
For Eq. (47), the following argument is made:
\begin{align*}
    \ip{A\xstar - y, Ae} &= \ip{A^\top (A\xstar - y), \xhat - \xstar} \geq 0
\end{align*}
The above is true by the fact that $\xstar$ is optimal for the
original problem and the Taylor expansion of $f$: suppose that, for some $z \in
\cC$, it held that
\[
    \ip{\grad f(\xstar), z - \xstar} < 0.
\]
The, by the Taylor expansion of $f$, we would get:
\[
    f(\xstar + \lambda (z - \xstar)) =
    f(\xstar) + \lambda \ip{\grad f(\xstar), z - \xstar} + o(\lambda)
    < f(\xstar),
\]
since $\lambda \ip{\grad f(\xstar), z - \xstar} + o(\lambda) < 0$ for
sufficiently small $\lambda > 0$. Then, $\xstar + \lambda (z - \xstar) \in \cC$
by the convexity of $\cC$, which would give us a contradiction by our assumption
on optimality. Hence, $\ip{\grad f(\xstar), \xhat - \xstar} \geq 0$ since we
know that $\xhat - \xstar \in \cK$ (i.e. a feasible direction).
This is why Eq. (47) holds.


\paragraph{Details for Lemma 2}
Here, we explain how to handle the various quantities that appear in Lemma 2.
Notice, for an element $v \in \cV_+$:
\begin{align*}
	u^\T Q v &= (u + v - v)^\T Q (u + v - u) \\
		&= (u + v)^\T Q (u + v) - u^\T Q u - v^\T Q u
		 - \cancel{v^\T Q u} - v^\T Q v + \cancel{v^\T Q u} \\
		&= (u + v)^\T Q (u + v) - u^\T Q u - v^\T Q v - v^\T Q u \Rightarrow \\
	2 u^\T Q v &= (u + v)^\T Q (u + v) - u^\T Q u - v^\T Q v \Rightarrow \\
	\abs{u^\T Q v} &\leq \frac{1}{2} \left(
		\abs{(u + v)^\T Q (u + v)} + \abs{u^\T Q u} + \abs{v^\T Q v}
	\right),
\end{align*}
where the last equality above follows from the fact that $Q = Q^\T$, which
implies that $v^\T Q u = u^\T Q v$. The reason we make the distinction in $V_+$
is briefly explained: we need to normalize the vector $(u + v)$, which gives us
\[
	\frac{u + v}{\norm{u + v}_2} =
	\frac{u + v}{\norm{u}^2 + \norm{v}^2 + 2 \ip{u, v}} =
	\frac{u + v}{2 + 2 \ip{u, v}}
\]
It is easy to see that, had we not made the distinction between $V_+, V_-$,
then we might as well have $\ip{u, v} = -\norm{u} \norm{v} = -1$ leading to a
zero denominator. Similary, when we are dealing with $v \in V_-$, we use a
different decomposition, that is $\frac{u - v}{\norm{u - v}_2}$, for the same
reason.

\paragraph{Details for Corollary 3a}
Note the following: when $X_i \sim \cN(0, \sigma_i^2)$, it holds
that~\cite[Eq. (3.13)]{LedTal13}:
\begin{equation}
    \expec{\max_{i \in [n]} \abs{X_i}} \leq 3 \sqrt{\log n} \max_{i \in [n]}
    \expec{X_i^2}^{1/2}
    \label{eq:expected_sup_gaussian}
\end{equation}

We have the following characterization for the tangent cone, when the support of
$\bar{x}$ is $S$:
\[
    T_{\set{z \in \Rbb^d: \norm{z}_1 \leq r}}(\bar{x})
    = \set{\Delta: \ip{\Delta_S, \mathrm{sign}(\bar{x}_S)}
           + \norm{\Delta_{S^c}}_1 \leq 0}
\]
An immediate consequence, lower bounding the above by the C-S inequality, is:
\begin{equation}
    \norm{\Delta_{S^c}}_1 \leq \norm{\Delta_S}_2 \norm{\mathrm{sign}(\bar{x}_S)}
    \leq \sqrt{\abs{S}} \norm{\Delta_S}_2.
    \label{eq:delta_ineq}
\end{equation}
Observe the next chain of inequalities:
\begin{align*}
    \norm{\Delta}_1 &= \norm{\Delta_S}_1 + \norm{\Delta_{S^c}}_1
    \overset{\norm{z}_1 \leq \sqrt{d} \norm{z}_2}{\leq}
        \sqrt{\abs{S}} \norm{\Delta_S}_2 + \norm{\Delta_{S^c}}_1 \\
    &\overset{\eqref{eq:delta_ineq}}{\leq}
        2 \sqrt{\abs{S}} \norm{\Delta_S}_2 \leq 2 \sqrt{\abs{S}} \norm{\Delta}_2
\end{align*}
Consider the case where $\norm{A \Delta}_2 = 1$: then we have that, since all
$\Delta \in \cK \Rightarrow \norm{\Delta}_1 \leq 2 \sqrt{k}$:
\[
    \frac{\norm{A \Delta}_2}{\norm{\Delta}_2} \geq \sqrt{\gamma_k^-(A)}
    \Rightarrow \norm{\Delta}_2 \leq \frac{1}{\sqrt{\gamma_k^-(A)}}.
\]

\paragraph{From S-Gaussian to Gaussian width}
We briefly address how the S-gaussian width,
\[
	\expec[S, g]{\sup_{z \in A \cK \cap \Sbb^{n-1}}
		\abs{\ip{g, \frac{Sz}{\sqrt{m}}}}}
\]

relates to the ordinary gaussian width. We consider the example of the
$\ell_1$-constrained least squares. Notice that we have the following chain of
inequalities, for $u$ such that $\norm{u}_1 \leq 2 \sqrt{k} \norm{u}_2,
\norm{Au}_2 = 1$:
\begin{align*}
	\ip{g, \frac{S A u}{\sqrt{m}}} &= \frac{1}{\sqrt{m}}\ip{A^\T S^\T g, u} \\
		&\overset{(\text{H{\"o}lder})}{\leq} \frac{\norm{u}_1}{\sqrt{m}}
		 \norm{A^\T S^\T g}_{\infty} = \frac{2 \sqrt{k}}{\sqrt{\gamma_k^-(A)}}
		 \frac{\norm{A^\T S^\T g}_{\infty}}{\sqrt{m}}
\end{align*}
Taking expectations gives us
\begin{align*}
	\expec[g, S]{\ip{g, \frac{S A u}{\sqrt{m}}}} &\leq
	\frac{2 \sqrt{k}}{\sqrt{\gamma_k^-(A)}}
	\frac{\expec[g, S]{\norm{A^\T S^\T g}_{\infty}}}{\sqrt{m}}
\end{align*}
Notice that the vector $S^\T g$ is itself Gaussian with
\[
	\frac{S^\T g}{\sqrt{m}} \sim \cN\left(0, \frac{S^\top S}{\sqrt{m}} \right).
\]
This gives us the inequality
\[
	\expec[g]{\frac{\norm{A^\T S^\T g}_{\infty}}{\sqrt{m}}}
	\leq 4 \max_{j=1, \dots, d} \frac{\norm{S a_j}_2}{\sqrt{m}}
		\sqrt{\log d},
\]
which follows from the same argument employed for the Gaussian tail bound in
the Corollary of the subgaussian sketch for $\ell_1$-constrained minimization.
Now, we refine the above by conditioning on the probability of $\norm{S a_j}_2$
being large for all vectors $a_j, \; j = 1, \dots, d$.
That probability has been bounded by Krahmer and Ward~\cite{KraWard11}, which
proved the following:
\begin{lemma}[Theorem 3.1 in~\cite{KraWard11}]
	\label{lemma:JL_embedding_rademacher}
	Fix $\eta > 0$, $\epsilon \in (0, 1)$, and consider a finite set $E
	\subseteq \Rbb^n$ such that $\abs{E} = p$. Set $k \geq 40
	\log\left(\frac{4p}{\eta}\right)$, and suppose that $\Phi \in \Rbb^{m \times
	n}$ satisfies
	\[
		(1 - \delta) \norm{x}_2^2 \leq \norm{\Phi x}_2^2 \leq
		(1 + \delta) \norm{x}_2^2, \; \forall x : \norm{x}_0 \leq k
	\]
	for $\delta \leq \frac{\epsilon}{4}$. Let $\varepsilon \in \Rbb^n$ be a
		Rademacher sequence. Then with probability exceeding $1 - \eta$, it
		holds that
	\[
		(1 - \epsilon) \norm{x}_2^2 \leq
		\norm{\Phi \Delta_{\varepsilon} x}_2^2 \leq
		(1 + \epsilon) \norm{x}_2^2, \;
		\forall x \in E.
	\]
\end{lemma}
Applying~\cref{lemma:JL_embedding_rademacher} to our case, where $p = d$, and
setting $\eta = \frac{1}{n}, \; \epsilon = 1$, gives us that
\[
	\mathcal{E} = \set{\norm{S a_j}_2 \leq 2 \sqrt{m} \norm{a_j}_2, \;
	\forall j \in [d]} \Rightarrow
	\prob{\mathcal{E}^c} \leq \frac{1}{n}.
\]
Then, it is simply a matter of conditioning on the event $\mathcal{E}^c$ and
observing that it always holds that $\frac{\norm{S a_j}_2}{\sqrt{m}} \leq \sqrt{n}
\norm{a_j}_2 \opnorm{\frac{H^\T D}{\sqrt{m}}} \leq \sqrt{n} \norm{a_j}_2$ to
obtain
\begin{align*}
	\expec[g, S]{\frac{\norm{A^\T S^\T g}_{\infty}}{\sqrt{m}}} &=
	\expec[S]{4 \max_{j=1,\dots,d} \frac{\norm{S a_j}_2}{\sqrt{m}}
		\sqrt{\log d} \mmid \mathcal{E}} \prob{\mathcal{E}} +
	\expec[S]{4 \max_{j=1, \dots, d} \frac{\norm{S a_j}_2}{\sqrt{m}}
		\sqrt{\log d} \mmid \mathcal{E}^c} \prob{\mathcal{E}^c} \\
    &\leq \left( 8 \max_{j = 1, \dots, d} \norm{a_j}_2
      + 4 \frac{\sqrt{n}}{n} \max_{j = 1, \dots, d} \norm{a_j}_2
      \right) \sqrt{\log d} \\
	&\leq 12 \max_{j = 1, \dots, d} \norm{a_j}_2 \sqrt{\log d}.
\end{align*}
The range for $m \geq c_0 \log^5 n \log d$ is such that the matrix $\Phi$
in~\cref{lemma:JL_embedding_rademacher} satisfies the desired Restricted
Isometry Property.

\bibliographystyle{plain}
\bibliography{references}

\end{document}
