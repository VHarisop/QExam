\documentclass[10pt]{beamer}

\usepackage{default}
\usepackage{xltxtra}
\usepackage{fontspec}
\usepackage{../latex-macros/macros/latex-macros}
\usepackage{cancel}
%\usefonttheme[onlymath]{serif}
\usefonttheme{professionalfonts}

\usepackage{svg}
\usepackage{pgf}
\usepackage{tikzducks, tikz}
\usetikzlibrary{arrows}
\usepackage{mathrsfs}
\usepackage[sort]{natbib}

\mode<presentation>
{
    \usetheme
    [navbar=true,colorblocks=true,pagenumbers=true]{Cornell}
}

\usepackage[skins]{tcolorbox}
\newcommand{\xstar}{x^{\star}}

\allowdisplaybreaks

\begin{document}

\title{Randomized Sketches of Convex Programs with Sharp Guarantees}
\subtitle{With the help of \texttt{tikzducks} and others}
\author{Mert Pilanci \& Martin Wainwright}
\frame{\titlepage}
\frame{\tableofcontents}

%% INTRODUCTION: Problem motivation, difficulties and prior art
\section{Introduction}
\begin{frame}{Overview: a blessing...}
    % Convex programs nice, forms with closed solutions, or iterative methods
    % with efficient steps that can be computed analytically
    \begin{itemize}
    \item<1-> Convex Optimization is fundamental in engineering,
    statistics, and other disciplines.
    \item<2-> Attractive property: convex programs can be solved to global
    optimality (in practice, $\epsilon$-close to the global optimum)
    \item<3-> Plethora of theoretical (e.g. convergence, optimality conditions)
    and practical results (e.g. algorithms, accelerated methods).
    \end{itemize}
\end{frame}
%
\begin{frame}{...and a Curse (of Dimensionality)}
    While many convex problems can be solved in polynomial time, \textit{not
    all polynomials are practical}.
    \linebreak
    \begin{block}<2->{An example: linear regression}
        Noisy linear measurements:
        \begin{align*}
            y_i &= \ip{a_i, x^*} + \eta_i, \; i = 1, \dots, n.
        \end{align*}
        Convex program to find least squares estimate:
        \begin{align*}
            \mbox{Minimize } & \norm{Ax - y}_2^2
        \end{align*}
        Solving via $QR$ factorization requires
        ($\sim \mathcal{O}(nd^2)$)
    \end{block}
\end{frame}


\begin{frame}{More generally: statistical estimation}
    \begin{itemize}
    \item Parameter estimation + prior information about parameter
    \item Low dimensional spaces: sparse vectors, low rank matrices, etc.
    \item The convex optimization way: relax constraints.
    \end{itemize}
    \begin{block}{Examples}
        \begin{enumerate}
            \item $s$-sparse vectors $\Rightarrow$ $\norm{x}_{1} \leq s$
            (also known as \textit{basis pursuit}) \\
            \item $\rank(A) \leq r$ $\Rightarrow$ $\norm{A}_{*} \leq r$
            (nuclear norm regularization)
        \end{enumerate}
    \end{block}

    \vspace{1em}

    Problem: ambient space of relaxation can be
    {\color{cred} too large}
\end{frame}

\section{A sketchy trick: random projections}
%% Overview of technique
\begin{frame}{Random projections}
    %TODO: Cite JL
    Random projections go back at least as far as 1984: Johnson and
    Lindenstrauss showed that we can project a set of $m$ points from $\Rbb^d$
    in a subspace of dimension $\Theta\left( \frac{\log m}{\epsilon^2} \right)$
    without distorting the distances between them more than $\epsilon$.
    \linebreak[2]
    \textbf{General idea}: project to low dimensional subspace and solve the
    problem efficiently there to obtain $\xhat$.
\end{frame}

\begin{frame}{Sketching for quadratic programs}
    Convex program:
    \[
        \xstar \in \argmin_{x \in \cC} \overbrace{\norm{Ax - y}_2^2}^{f(x)},
        \quad A \in \Rbb^{n \times d}, y \in \Rbb^n.
    \]
    \textbf{\color{cred} Sketched} program:
    \[
        \xhat \in \argmin_{x \in \cC}
        \underbrace{\norm{{\color{cred} S}(Ax - y)}_2^2}_{g(x)},
        \quad S \in \Rbb^{m \times n}, \; \underbrace{m \ll
            \min\set{n, d}}_{\text{ideally}}
    \]
    \only{\begin{figure}[h]
        \centering
        \begin{tikzpicture}[scale=1.2]
            \duck[think={\tiny{How small $m$?}}, bubblecolour=
            white!35!cred!25,
            squareglasses=blue!50!black]
        \end{tikzpicture}
    \end{figure}}<2->
\end{frame}

\begin{frame}{Sketching matrices}
    Different choices of sketching matrix $S$:
    \begin{itemize}
    \item \textbf{subgaussian} sketch: rows $s_i$ are i.i.d. subgaussian, i.e.
    \[
        \prob{\abs{\ip{s_i, u}} \geq t} \leq 2e^{-\frac{t^2}{2 \sigma^2}}, \;
        \forall t > 0.
    \]
    Satisfied by Gaussian or Rademacher vectors
    \item \textbf{randomized orthogonal} sketch: $s_i$ are i.i.d. orthonormal
    rows satisfying $s_i = \sqrt{n} D H^\top p_i$, where $D$ a random
    diagonal matrix, $H$ the Hadamard matrix, and $p_i$ chosen uniformly from
    the canonical basis of $\Rbb^n$.
    \end{itemize}
    Will examine subgaussian case in detail.
\end{frame}


%% Fundamental dependence
\begin{frame}{Key quantities}
    \only<1>{
    \begin{block}{Tangent cone $\cK$}
        Given a constraint set $\cC \subseteq \Rbb^d$, the cone of all feasible
        directions from the optimum $\xstar \in \cC$ is defined as
        \[
            \cK := \mathrm{clconv}\set{
                z \in \Rbb^d \mmid z = t (x - \xstar), \; t \geq 0, \;
                x \in \cC
            }
        \]
    \end{block}
    Since the objective function is $\norm{Ax - y}_2^2$, we need to examine the
    \textbf{transformed} cone:
    \[
        A \cK := \set{A z \in \Rbb^n \mmid z \in \cK}
    \]}
    \only<2>{
    \begin{figure}[h]
        \centering
        \includesvg[width=0.5\linewidth]{geogebra-export}
        \caption{rays of $\cK$ shown in black}
    \end{figure}}
    \only<3>{
        \begin{block}{Gaussian width}
            Given a set $S \subseteq \Rbb^n$, we define its \textbf{Gaussian
            width} as
            \[
                \mathbb{W}(S) := \expec[g]{\sup_{z \in S}
                \abs{\ip{g, z}}}, \quad g \sim \cN(0, I_n)
            \]
        \end{block}
        \textbf{Interpretation}: in a probabilistic scenario, sets with large
        gaussian width will exhibit more degrees of freedom.

        \begin{center}
        Quantity of interest: \fcolorbox{black}{red!20}{\centering
        $
            \displaystyle
            \mathbb{W}(A\cK) := \expec[g]
            {\sup_{z \in A\cK \cap \cS^{n-1}} \abs{\ip{g, z}}}
        $}
        \end{center}
    }
\end{frame}

%% MAIN: Presentation of main result(s), what is guaranteed and what's not
\section{Main results}

\subsection{Subgaussian case}

\begin{frame}{Main results - in a nutshell}
    \begin{itemize}
    \item<1-> $m$ should not depend on the ambient dimension, but rather the
        \textbf{statistical} dimension!
    \item<2-> if the sketching matrix is structured, expect to pay a small price
        for the randomness lost
    \end{itemize}
    \vspace{1em}
    \only{\centering $\delta$-optimality:
        \fcolorbox{black}{cred!20}{
            $f(\hat{x}) \leq (1 + \delta)^2 f(\xstar)$
        }}<3->
\end{frame}

\begin{frame}{Main result - subgaussian sketch}
    \begin{block}{Theorem 1}
        Pick $S \in \Rbb^{m \times n}$ according to the subgaussian model.
        There exist universal constants $c_0, c_1,
        c_2 > 0$ such that, $\forall \delta \in (0, 1)$, when
        \[
            m \geq \frac{c_0}{\delta^2} \mathbb{W}^2(A \cK), \]
        the sketched solution is $\delta$-optimal with probability at least
        \(
            1 - c_1 e^{-c_2 m \delta^2}.
        \)
    \end{block}
    \vspace{1em}
    \begin{columns}
        \begin{column}{0.49 \textwidth}
            \centering
            \only{
            \begin{tikzpicture}[scale=1.1]
                \duck[speech={\scriptsize So what?},
                      bubblecolour=white!35!cred!25,
                      squareglasses=blue!50!black]
            \end{tikzpicture}}<2-3>
            \only{
            \begin{tikzpicture}[scale=1.1]
                \duck[speech={\tiny $
                    \substack{\text{Sounds sketchy,}\\\text{but OK}}$},
                      bubblecolour=white!35!cred!25,
                      squareglasses=blue!50!black]
            \end{tikzpicture}}<4>
        \end{column}
        \begin{column}{0.49 \textwidth}
            \only{
            \begin{itemize}
                \item always true: $\mathbb{W}^2(A \cK) \leq n$
                \item for many sets $A \cK$, \textbf{much} smaller!
                (e.g. low rank matrices)
                \end{itemize}}<3->
        \end{column}
    \end{columns}
\end{frame}

\section{Some concrete examples}

\begin{frame}{Unconstrained least squares}
    Consider the sketched problem:
    \begin{align*}
        \inf_{x \in \Rbb^n} \norm{S(Ax - y)}_2^2
    \end{align*}
    Intuition: complexity of the problem depends on $\rank(A)$ \linebreak

    \begin{center}
        {\color{cred} \textbf{How big is} $\mathbb{W}(A \cK)$?}
        \only{
            \begin{tikzpicture}[remember picture, overlay]
                \node[opacity=0.5,inner sep=1pt] at (current page.south)
                {\includegraphics[width=0.7\paperwidth,height=0.7\paperheight]
                {images/homer_woohoo.jpg}};
            \end{tikzpicture}
            Less than $\sqrt{\rank(A)}$!
        }<2->
    \end{center}
\end{frame}


\begin{frame}{Unconstrained least squares}
    \begin{block}{Corollary 2(a) in~\cite{PilWain15}}
        Consider the problem $\inf_{x \in \Rbb^n} \norm{Ax - y}_2^2$ and its
        sketched version $\inf_{x \in \Rbb^n} \norm{S(Ax - y)}_2^2, \;
        S \in \Rbb^{m \times n}$. If $m \geq c_0 \frac{\rank(A)}{\delta^2}$, the
        sketched solution satisfies
        \[
            \prob{f(\xhat) \leq (1 + \delta)^2 f(\xstar)}
            \geq 1 - c_1 \exp\left(-c_2 m \delta^2\right).
        \]
    \end{block}
    \vspace{1em}

    \textbf{Proof sketch}:
        Write $Au = \sum_{i=1}^{\rank(A)} \lambda_i a_i$, where
        $\set{a_i}_{i=1}^{\rank(A)}$ is an orthonormal basis of
        $\mathrm{im}(A)$. Plug into the definition of $\mathbb{W}(A \cK)$
        and apply Theorem 1.

    \vspace{1em}

    \only{	\begin{center}
    \fcolorbox{black}{cred!20}{
    Solving sketched problem via QR: $\mathcal{O}(ndm + md^2)$}
    \end{center} }<2>
\end{frame}


\begin{frame}{$\ell_1$-constrained least squares}
    Consider one of the equivalent formulations of the LASSO, given by
    \[
        \xstar \in \argmin_{\norm{x}_1 \leq R} \norm{Ax - y}_2^2
    \]
    Case of interest: $\xstar$ sparse ($\abs{\set{i \in [d]: x_i \neq 0}} = k
    \ll
    d$),
    unique.
    \vspace{1em}
    \begin{center}
        \begin{tcolorbox}[colback=cred!20, colframe=cred,
            title={\textbf{Key quantity}: $\ell_1$-restricted eigenvalue},
            boxrule=0.2mm, width=(0.75 \linewidth)]
        \begin{align}
            \begin{aligned}
                \gamma_k^-(A) & \triangleq
                    \inf_{\substack{\norm{z}_2 = 1 \\ \norm{z}_1 \leq 2
                    \sqrt{k}}} \norm{Az}_2^2
            \end{aligned}
            \label{eq:ell_1_eigs}
        \end{align}
        \end{tcolorbox}
    \end{center}
\end{frame}

\begin{frame}{Calculating $\mathbb{W}(A \cK)$ - 1}
    \begin{itemize}
        \item
        Tangent cone: $\cK = \set{\Delta : \ip{\Delta_S,
        \mathrm{sign}(x^*_S)} + \norm{\Delta_{S^c}}_1 \leq 0}$,
        where $S$ support of $\xstar$. By C-S, gives
        \begin{equation}
            \norm{\Delta_{S^c}}_1 \leq \norm{\Delta_S}_2
            \underbrace{\norm{\mathrm{sign}(\xstar_S)}_2}_{= \sqrt{k}}
            \label{eq:delta_ineq}
        \end{equation}

        \item Chain of (in)equalities:
        \begin{align*}
            \norm{\Delta}_1 &= \norm{\Delta_S}_1 + \norm{\Delta_{S^c}}_1
            \leq \sqrt{k} \norm{\Delta_S}_2 +
            \underbrace{\sqrt{k} \norm{\Delta_{S}}_2}_{
                \text{from } \eqref{eq:delta_ineq}}
            \leq 2 \sqrt{k} \norm{\Delta}_2
        \end{align*}
        \item $\underbrace{\norm{A \Delta}_2 = 1}_{A \Delta \in \Sbb^{n-1}}
        \Rightarrow
            \norm{\Delta}_2 \leq \frac{1}{\sqrt{\gamma_{k}^-(A)}}$,
            from definition of $\gamma_k^-(A)$.
    \end{itemize}
\end{frame}
\begin{frame}{Calculating $\mathbb{W}(A \cK)$ - 2}
    Combine all of the above to obtain
    \begin{align*}
        \expec{\abs{\ip{A \Delta, g}}} &= \expec{\abs{\ip{\Delta, A^\top g}}}
        \overset{(\text{H{\"o}lder})}{\leq}
        \expec{\norm{\Delta}_1
        \infnorm{A^\top g}} \\
        & \leq \frac{2 \sqrt{k} \expec{\infnorm{A^\top g}}}{
        \sqrt{\gamma_k^-(A)}} =
        \frac{2 \sqrt{k}}{\sqrt{\gamma_k^-(A)}}
        \expec{\max_{i \in [d]} \abs{a_i^\top g}}
    \end{align*}
    Observe: $a_i^\top g \sim \cN(0, \norm{a_i}_2^2)$ so by a standard
    argument~\cite[Eq. (3.13)]{LedTal13}
    \[
        \mathbb{W}(A \cK) \leq 6 \sqrt{\frac{k \log d}{\gamma_k^-(A)}}
        \max_{i \in [d]} \norm{a_i}_2
    \]
\end{frame}

\begin{frame}{Calculating $\mathbb{W}(A \cK)$ - 3}
    Finally: $A \cK \subseteq \mathrm{Im}(A)$ $\Rightarrow$ bound from
    unconstrained LS still valid!
    \[
        \mathbb{W}(A\cK) \leq \min\set{\sqrt{\rank(A)},
        2 \sqrt{\frac{k \log d}{\gamma_k^-(A)}} \max_{i \in [d]} \norm{a_i}_2}
    \]
    \begin{block}{Corollary 3a in~\cite{PilWain15}}
        Consider a sub-gaussian sketch applied to the $\ell_1$-constrained
        least squares. The solution is $\delta$-optimal for
        \[
            m \geq \frac{c_0}{\delta^2} \min\set{\rank(A),
            \left( 6 \sqrt{\frac{k \log d}{\gamma_k^-(A)}} \max_{i \in [d]}
            \norm{a_i}_2 \right)^2}.
        \]
    \end{block}
\end{frame}

\section{Proof sketch of main result}

\begin{frame}{Proving Theorem 1 - Overview}
    Proof of Theorem 1 consists of 2 conceptually simple steps:
    \begin{enumerate}
    \item<1-> A deterministic lemma relating $f(\xhat)$ to $f(\xstar)$, which
        depends on two random quantities, using convex optimality conditions
    \item<2-> Bounding said quantities with high probability, using
        concentration inequalities and empirical process theory
    \end{enumerate}
    \only{
    \begin{center}
    \begin{tcolorbox}[colback=cred!20, colframe=cred,
        title={Key quantities:},
        boxrule=0.2mm, width=(\linewidth)]
        \begin{align}
            \begin{aligned}
                Z_1(A \cK) &:= \inf_{v \in A\cK \cap \Sbb^{n-1}}
                    \frac{1}{m} \norm{Sv}_2^2 \\
                Z_2(A \cK) &:= \sup_{v \in A\cK \cap \Sbb^{n-1}}
                    \abs{\ip{u, \left(
                        \frac{1}{m} S^\top S - I_n
                    \right) v}} , \; u \in \Sbb^{n - 1}.
            \end{aligned}
            \label{eq:z1_z2_def}
        \end{align}
    \end{tcolorbox}
    \end{center}}<2->
\end{frame}


\begin{frame}{From $f(\xhat)$ to $f(\xstar)$}
\only{
    Step (1a): define $e = \xhat - \xstar$, rewrite $f(\xhat)$ + triangle ineq.:
    \[
        f(\xhat) \leq \left(1 + \frac{\norm{Ae}_2}{\norm{A\xstar - y}_2}
        \right)^2 f(\xstar).
    \]
    Step (1b): write optimality conditions for original and sketched problem to
    obtain
    \[
        \frac{1}{2m} \norm{SAe}_2^2 \leq \abs{
            \ip{A \xstar - y,
            \underbrace{\left( \frac{1}{m} S^\top S - I_n\right)}_{=: Q}
            A e}
        }
    \]
    $e$ belongs to $\cK$ since both original and sketched problem have same
    constraints, leading to
    \[
        \frac{1}{2} Z_1(A \cK) \norm{A e}_2 \leq
        \norm{A \xstar - y}_2 Z_2(A \cK).
    \]}<1>
\only{
    We've proved the following deterministic lemma:
    \begin{block}{Lemma 1}
        For any choice of sketching matrix $S \in \Rbb^{m \times n}$, we have
        \[
            f(\xhat) \leq \left(1 + 2 \frac{Z_2(A \cK)}{Z_1(A \cK)} \right)^2
            f(\xstar).
        \]
    \end{block}
}<2>
\end{frame}

\begin{frame}{Controlling $Z_1, Z_2$}
    To show that the ratio in Lemma 1 is small w.h.p, sufficient to bound $Z_1$
    from below and $Z_2$ from above.

    Proof depends on following result:
    \begin{block}{Proposition 1 (follows Theorem D from \cite{MenPajTom07})}
        Let $\set{s_i}_{i=1}^m$ i.i.d from an isotropic $\sigma$-subgaussian
        distr. $\exists c_1, c_2 > 0$ such that for any
        $ \cY \subseteq \Sbb^{n - 1} $:
        \[
            \prob{\sup_{y \in \cY} \abs{y^\top \left(\frac{1}{m} S^\top S - I_n
            \right) y} \leq c_1 \frac{\mathbb{W}(\cY)}{\sqrt{m}} + \delta}
            \geq 1 - e^{-\frac{c_2 m \delta^2}{\sigma^4}}.
        \]
    \end{block}
\end{frame}

\begin{frame}{Step (2a) - controlling $Z_1$ }
    Controlling $Z_1$ is easy - apply Prop. 1 with $\cY = A\cK \cap \Sbb^{n-1}$:
    \begin{align*}
        \forall v \in A \cK \cap \Sbb^{n-1}:
        \frac{1}{m} \norm{Sv}^2_2 - \cancelto{1}{v^\top v}
        & \geq - c_1 \frac{\mathbb{W}(A \cK)}{\sqrt{m}} - \delta \\
        \Rightarrow
        \inf_{v \in A\cK \cap \Sbb^{n-1}} \frac{1}{m} \norm{Sv}_2^2 \geq 1 -
        c_1 \frac{\mathbb{W}(A
        \cK)}{\sqrt{m}} - \delta
    \end{align*}
    Set $m \geq \left( \frac{\sqrt{c_1} \mathbb{W}(A \cK)}{\delta} \right)^2$.
    \qed
\end{frame}

\begin{frame}{Step (2b) - controlling $Z_2$}
\begin{columns}
    \begin{column}{0.75 \textwidth}
    Controlling $Z_2(A \cK)$ is more involved: $Z_2$ involves two vectors $u,
    v$, which dictate a decomposition of $Z_2$ s.t. Prop. 1 is applicable:
    \[
        Z_2(A \cK) \leq \sup_{v \in \cV_+} \abs{u^\top Q v} +
        \sup_{v \in \cV_-} \abs{u^\top Q v}
    \]
    For $v \in \cV_+$, wlog:
    \[
        \abs{u^\top Q v} \leq
        \underbrace{\frac{1}{2} \abs{(u + v)^\top Q (u + v)}}_{
            \text{only nontrivial term}}
        + \underbrace{\frac{1}{2} \abs{u^\top Q u}}_{\cY \gets \set{u}} +
        \underbrace{\frac{1}{2} \abs{v^\top Q v}}_{\cY \gets \cV_+}
    \]
    \end{column}
    \vrule{}
    \begin{column}{0.24 \textwidth}
        \begin{align*}
            \scalebox{0.8}{$Q := \frac{S^\top S}{m} - I_n$} \\
            \scalebox{0.8}{$V_+ = \set{v \in A \cK : \ip{u, v} \geq 0}$} \\
            \scalebox{0.8}{$V_- = \set{v \in A \cK : \ip{u, v} < 0}$}
        \end{align*}
    \end{column}
\end{columns}
\end{frame}

\begin{frame}{Step (2b) - controlling the nontrivial term}
    Define $\cU_+ := \set{\frac{u + v}{\norm{u + v}} \mmid v \in \cV_+}$ and
    apply Prop. 1 with $\cY = \cU_+$:
    \[
        \sup_{v \in \cV_+}
        \frac{\abs{(u + v)^\top Q (u + v)}}{
        \underbrace{\norm{u + v}^2}_{\leq 4}}
        \leq c_1 \frac{\mathbb{W}(\cU_+)}{\sqrt{m}} + \delta.
    \]
    Split $v$ into a $u$-parallel and a $u$-orthogonal component, $\Pi(v)$, to
    obtain
    \[
        \underbrace{\expec{\sup_{v \in \cV_+} \abs{\ip{g, \frac{u+v}{\norm{u +
        v}}}}}}_{= \mathbb{W}(\cU_+)}
        \leq \underbrace{\expec{\abs{\ip{g, u}}}}_{= \sqrt{\frac{2}{\pi}}} +
        \underbrace{\expec{\sup_{v \in \cV_+} \abs{\ip{g, \Pi(v)}}}}_{
        \leq ?}
    \]
\end{frame}

\begin{frame}{Step (2b) - controlling the nontrivial term}
    For the last term, note that
    \begin{align*}
        \expec{\ip{g, \Pi(v) - \Pi(v')}^2} &=
        (\Pi(v) - \Pi(v'))^\top \underbrace{\expec{g g^\top}}_{= I} (\Pi(v) -
        \Pi(v')) \\
        &\leq \norm{v - v'}^2 = \expec{\left( \ip{g, v - v'} \right)^2}
    \end{align*}
    Using the Sudakov-Fenrique inequality\footnote{see e.g.~\cite[Theorem
    7.2.11]{Versh18}}, we deduce that
    \[
        \expec{\sup_{v \in \cV_+} \abs{\ip{g, \Pi(v)}}}
        \leq \expec{\sup_{v \in \cV_+} \abs{\ip{g, v}}} = \mathbb{W}(\cV_+)
        \leq \mathbb{W}(A \cK \cap \Sbb^{n - 1})
    \]
    which completes the bounding of $\mathbb{W}(\cU_+)$.
\end{frame}

\begin{frame}{The duck says...}
    \centering
    \begin{tikzpicture}[scale=3]
        \duck[speech={\scalebox{0.8}{What about ROS sketches?}},
              bubblecolour=white!35!cred!25,
              squareglasses=blue!50!black]
    \end{tikzpicture}
\end{frame}

\begin{frame}{What about ROS sketches?}
    In a nutshell: faster computations, not so sharp guarantees for $m$.
    Different quantities at play:
    \begin{itemize}
    \item<2-> \textit{S-Gaussian width}:
            \fcolorbox{black}{red!20}{\centering
            $
                \displaystyle
                \mathbb{W}_S(A\cK) := \expec[g, S]
                {\sup_{z \in A\cK \cap \cS^{n-1}} \frac{1}{\sqrt{m}}
                   \abs{\ip{g, SAz}}}
            $} \\
        $\triangleright$ can be close to $\mathbb{W}(A \cK)$.
    \item<3-> \textit{Rademacher complexity}:
            \fcolorbox{black}{red!20}{\centering
            $
                \displaystyle
                \mathbb{R}(A\cK) := \expec[\varepsilon]
                {\sup_{z \in A\cK \cap \cS^{n-1}} \abs{\ip{\varepsilon, z}}}
            $} \\
        $\triangleright$ bounded above by a constant times $\mathbb{W}(A \cK)$.
    \end{itemize}
    \vspace{1em}
    \only{
    Caveat: $
    \frac{m}{\log m} = \Omega\left\{\left( \mathbb{R}^2(A \cK) + \log n \right)
    \mathbb{W}^2_S(A \cK)\right\} $, best $\cK$-agnostic bound obtained in
    paper}<4>
\end{frame}

\begin{frame}{Sharpening bounds for ROS sketches}
    Obtaining sharper bounds requires tangent cone $\cK$ to be more structured.
    \vspace{1em}

    Example - $\cK$ is a subspace:
    \begin{itemize}
    \item $Z_1(A \cK) \geq 1 - \delta, Z_2(A \cK) \leq \frac{3}{2} \delta$,
    as long as $m = \Omega(\log^4 n \cdot \rank(A))$
    \item \textbf{Proof sketch}: relax $A \cK \cap \Sbb^{n-1}$ to
    $A \cK \cap \mathbb{B}_2^n$, take $\epsilon$-net, and appeal to a JL
    property for finite sets of points from~\cite{KraWard11}
    \end{itemize}

    General pattern for sharpening bounds (not covered here): reduction to
    finite maxima.
\end{frame}

\section{Discussion}

\begin{frame}{Disclaimer}
    \begin{itemize}
        \item We described a randomized method to $(1 + \delta)^2$ approximate
        the optimal \textit{\color{cred} cost} with high probability
        \item We did \textbf{not} give any guarantees about approximating the
        optimal \textit{\color{blue} solution}, $\xstar$
    \end{itemize}
    Addressed in other works, e.g. Iterative Hessian sketch~\cite{PilWain16},
    Newton sketch~\cite{PilWain17}.

    \vspace{1em}

    \only{
    \begin{center}
        \fcolorbox{black}{green!20}{
        Takeaway: \textit{randomized methods work!} (pro\{v, b\}ably)}
    \end{center}}<2>
\end{frame}

\begin{frame}{~}
    \begin{center}
        \Large Thank you!
    \end{center}
\end{frame}

\begin{frame}[allowframebreaks]{References}
\bibliographystyle{plain}
\bibliography{references}
\end{frame}

\end{document}
