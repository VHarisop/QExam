\documentclass[10pt]{beamer}

\usepackage{default}
\usepackage{xltxtra}
\usepackage{fontspec}
\usepackage{../latex-macros/macros/latex-macros}
%\usefonttheme[onlymath]{serif}
\usefonttheme{professionalfonts}

\usepackage{svg}
\usepackage{pgf}
\usepackage{tikzducks, tikz}
\usetikzlibrary{arrows}
\usepackage{mathrsfs}
\usepackage[sort]{natbib}
\bibliographystyle{humannat}
\mode<presentation>
{
    \usetheme
    [navbar=true,colorblocks=true,pagenumbers=true]{Cornell}
}

\usepackage[skins]{tcolorbox}

\newcommand{\xstar}{x^{\star}}

\begin{document}

\title{Randomized Sketches of Convex Programs with Sharp Guarantees}
\frame{\titlepage}
\frame{\tableofcontents}

%% INTRODUCTION: Problem motivation, difficulties and prior art
\section{Introduction}
\begin{frame}{Overview: a blessing...}
    % Convex programs nice, forms with closed solutions, or iterative methods
    % with efficient steps that can be computed analytically
    \begin{itemize}
    \item<1-> Convex Optimization is a fundamental tool in engineering,
    statistics, and other disciplines.
    \item<2-> Attractive property: convex programs can be solved to global
    optimality (in practice, $\epsilon$-close to the global optimum)
    \item<3-> Plethora of theoretical (e.g. convergence, optimality conditions)
    and practical results (e.g. algorithms, accelerated methods).
    \end{itemize}
\end{frame}
%
\begin{frame}{...and a Curse (of Dimensionality)}
    While many convex problems can be solved in polynomial time, \textit{not
    all polynomials were created equal}.
    \linebreak
    \begin{block}<2->{An example: linear regression}
        Noisy linear measurements:
        \begin{align*}
            y_i &= \ip{a_i, x^*} + \eta_i, \; i = 1, \dots, n.
        \end{align*}
        Convex program to find least squares estimate:
        \begin{align*}
            \mbox{Minimize } & \norm{Ax - y}_2^2
        \end{align*}
        Closed-form solution, but requires matrix inversion
        ($\sim \mathcal{O}(n^3)$)
    \end{block}
\end{frame}


\begin{frame}{More generally: statistical estimation}
    \begin{itemize}
    \item Parameter estimation + prior information about parameter
    \item Low dimensional spaces: sparse vectors, low rank matrices, etc.
    \item The convex optimization way: relax constraints.
    \end{itemize}
    \begin{block}{Examples}
        \begin{enumerate}
            \item $s$-sparse vectors $\Rightarrow$ $\norm{x}_{1} \leq s$
            (also known as \textit{basis pursuit}) \\
            \item $\rank(A) \leq r$ $\Rightarrow$ $\norm{A}_{*} \leq r$
            (nuclear norm regularization)
        \end{enumerate}
    \end{block}
    In the examples above, the ambient space of the relaxation might be
    too large!
\end{frame}

\section{A sketchy trick: random projections}
%% Overview of technique
\begin{frame}{Random projections}
    %TODO: Cite JL
    Random projections go back at least as far as 1984: Johnson and
    Lindenstrauss showed that we can project a set of $m$ points from $\Rbb^d$
    in a subspace of dimension $\Theta\left( \frac{\log m}{\epsilon^2} \right)$
    without distorting the distances between them more than $\epsilon$.
    \linebreak[2]
    \textbf{General idea}: project to low dimensional subspace and solve the
    problem efficiently there to obtain $\xhat$.
\end{frame}

\begin{frame}{Sketching for quadratic programs}
    Convex program:
    \[
        \xstar \in \argmin_{x \in \cC} \overbrace{\norm{Ax - y}_2^2}^{f(x)},
        \quad A \in \Rbb^{n \times d}, y \in \Rbb^n.
    \]
    \textbf{\color{cred} Sketched} program:
    \[
        \xhat \in \argmin_{x \in \cC} \norm{{\color{cred} S}(Ax - y)}_2^2,
        \quad S \in \Rbb^{m \times n}, \; m < n.
    \]
    \only{\begin{figure}[h]
        \centering
        \begin{tikzpicture}[scale=1.2]
            \duck[think={\tiny{How small $m$?}}, bubblecolour=
            white!35!cred!25,
            squareglasses=blue!50!black]
        \end{tikzpicture}
    \end{figure}}<2->
\end{frame}

\begin{frame}{Sketching matrices}
    Different choices of sketching matrix $S$:
    \begin{itemize}
    \item \textbf{subgaussian} sketch: rows $s_i$ are i.i.d. subgaussian, i.e.
    \[
        \prob{\abs{\ip{s_i, u}} \geq t} \leq 2e^{-\frac{t^2}{2 \sigma^2}}, \;
        \forall t > 0.
    \]
    E.g. Gaussian or Rademacher vectors
    \item \textbf{randomized orthogonal} sketch: $s_i$ are i.i.d. orthonormal
    rows satisfying $s_i = \sqrt{n} D H^\top p_i$, where $D$ a random
    diagonal matrix, $H$ the Hadamard matrix, and $p_i$ chosen uniformly from
    the canonical basis of $\Rbb^n$.
    \end{itemize}
\end{frame}


%% Fundamental dependence
\begin{frame}{Key quantities}
    \only<1>{
    \begin{block}{Tangent cone $\cK$}
        Given a constraint set $\cC \subseteq \Rbb^d$, the cone of all feasible
        directions from the optimum $\xstar \in \cC$ is defined as
        \[
            \cK := \mathrm{clconv}\set{
                z \in \Rbb^d \mmid z = t (x - \xstar), \; t \geq 0, \;
                x \in \cC
            }
        \]
    \end{block}
    Since the objective function is $\norm{Ax - y}_2^2$, we need to examine the
    \textbf{transformed} cone:
    \[
        A \cK := \set{A z \in \Rbb^n \mmid z \in \cK}
    \]}
    \only<2>{
    \begin{figure}[h]
        \centering
        \includesvg[width=0.5\linewidth]{geogebra-export}
        \caption{rays of $\cK$ shown in black}
    \end{figure}}
    \only<3>{
        \begin{block}{Gaussian width}
            Given a set $S \subseteq \Rbb^n$, we define its \textbf{Gaussian
            width} as
            \[
                \mathbb{W}(S) := \expec[g]{\sup_{z \in S}
                \abs{\ip{g, z}}}, \quad g \sim \cN(0, I_n)
            \]
        \end{block}
        \textbf{Interpretation}: in a probabilistic scenario, sets with large
        gaussian width will exhibit more degrees of freedom.

        \begin{center}
        Quantity of interest: \fcolorbox{black}{red!20}{\centering
        $
            \displaystyle
            \mathbb{W}(A\cK) := \expec[g]
            {\sup_{z \in A\cK \cap \cS^{n-1}} \abs{\ip{g, z}}}
        $}
        \end{center}
    }
\end{frame}

%% MAIN: Presentation of main result(s), what is guaranteed and what's not
\section{Main result}

\begin{frame}{Main results - in a nutshell}
    \begin{itemize}
    \item<1-> $m$ should not depend on the ambient dimension, but rather the
        \textbf{statistical} dimension!
    \item<2-> if the sketching matrix is structured, expect to pay a small price
        for the randomness lost
    \end{itemize}
    \vspace{1em}
    \only{\centering $\delta$-optimality:
        \fcolorbox{black}{cred!20}{
            $f(\hat{x}) \leq (1 + \delta)^2 f(\xstar)$
        }}<3->
\end{frame}

\begin{frame}{Main result - subgaussian sketch}
    \begin{block}{Theorem 1}
        Pick $S \in \Rbb^{m \times n}$ according to the subgaussian model.
        There exist universal constants $c_0, c_1,
        c_2 > 0$ such that, $\forall \delta \in (0, 1)$, when
        \[
            m \geq \frac{c_0}{\delta^2} \mathbb{W}^2(A \cK), \]
        the sketched solution is $\delta$-optimal with probability at least
        \(
            1 - c_1 e^{-c_2 m \delta^2}.
        \)
    \end{block}
    \vspace{1em}
    \begin{columns}
        \begin{column}{0.49 \textwidth}
            \centering
            \only{
            \begin{tikzpicture}[scale=1.1]
                \duck[speech={\scriptsize So what?},
                      bubblecolour=white!35!cred!25,
                      squareglasses=blue!50!black]
            \end{tikzpicture}}<2-3>
            \only{
            \begin{tikzpicture}[scale=1.1]
                \duck[speech={\tiny $
                    \substack{\text{Sounds sketchy,}\\\text{but OK}}$},
                      bubblecolour=white!35!cred!25,
                      squareglasses=blue!50!black]
            \end{tikzpicture}}<4>
        \end{column}
        \begin{column}{0.49 \textwidth}
            \only{
            \begin{itemize}
                \item always true: $\mathbb{W}^2(A \cK) \leq n$
                \item for many sets $A \cK$, \textbf{much} smaller!
                (e.g. low rank matrices)
                \end{itemize}}<3->
        \end{column}
    \end{columns}
\end{frame}

%\begin{frame}{Main result - ROS sketch}
%    A similar result holds for randomized orthonormal systems (ROS), with
%    an extra $\log $ factor:
%    \begin{block}{Theorem 2}
%
%    \end{block}
%\end{frame}

\section{Some concrete examples}

\begin{frame}{Unconstrained least squares}
    Consider the sketched problem:
    \begin{align*}
        \inf_{x \in \Rbb^n} \norm{S(Ax - y)}_2^2
    \end{align*}
    Intuition: complexity of the problem depends on $\rank(A)$ \linebreak

    \begin{center}
        {\color{cred} \textbf{How big is} $\mathbb{W}(A \cK)$?} \\
        \only{\textbf{\color{cred} Answer}: Less than $\sqrt{\rank(A)}$!}<2->
    \end{center}
\end{frame}

\begin{frame}{Unconstrained least squares}
    \begin{block}{Corollary 2(a) in~\cite{PilWain15}}
        Consider the problem $\inf_{x \in \Rbb^n} \norm{Ax - y}_2^2$ and its
        sketched version $\inf_{x \in \Rbb^n} \norm{S(Ax - y)}_2^2, \;
        S \in \Rbb^{m \times n}$. If $m \geq c_0 \frac{\rank(A)}{\delta^2}$, the
        sketched solution satisfies
        \[
            \prob{f(\xhat) \leq (1 + \delta)^2 f(\xstar)}
            \geq 1 - c_1 \exp\left(-c_2 m \delta^2\right).
        \]
    \end{block}
    \vspace{1em}

    \textbf{Proof sketch}:
        Write $Au = \sum_{i=1}^{\rank(A)} \lambda_i a_i$, where
        $\set{a_i}_{i=1}^{\rank(A)}$ is an orthonormal basis of
        $\mathrm{im}(A)$. Plug into the definition of $\mathbb{W}(A \cK)$
        and apply Theorem 1.
\end{frame}


\begin{frame}{$\ell_1$-constrained least squares}
    Consider one of the equivalent formulations of the LASSO, given by
    \[
        \xstar \in \argmin_{\norm{x}_1 \leq R} \norm{Ax - y}_2^2
    \]
    Case of interest: $\xstar$ sparse ($\abs{\set{i \in [d]: x_i \neq 0}} = k
    \ll
    d$),
    unique.
    \vspace{1em}
    \begin{center}
        \begin{tcolorbox}[colback=cred!20, colframe=cred,
            title={\textbf{Key quantities}: $\ell_1$-restricted eigenvalues},
            boxrule=0.2mm, width=(0.75 \linewidth)]
        \begin{align}
            \begin{aligned}
                \gamma_k^-(A) & \triangleq
                    \inf_{\substack{\norm{z}_2 = 1 \\ \norm{z}_1 \leq 2
                    \sqrt{k}}} \norm{Az}_2^2 \\
                \gamma_k^+(A) & \triangleq
                    \sup_{\substack{\norm{z}_2 = 1 \\ \norm{z}_1 \leq
                    2 \sqrt{k}}} \norm{Az}_2^2
            \end{aligned}
            \label{eq:ell_1_eigs}
        \end{align}
        \end{tcolorbox}
    \end{center}
\end{frame}

\begin{frame}{Calculating $\mathbb{W}(A \cK)$ - 1}
    \begin{itemize}
        \item
        Tangent cone: $\cK = \set{\Delta : \ip{\Delta_S,
        \mathrm{sign}(x^*_S)} + \norm{\Delta_{S^c}}_1 \leq 0}$,
        where $S$ support of $\xstar$. By C-S, gives
        \begin{equation}
            \norm{\Delta_{S^c}}_1 \leq \norm{\Delta_S}_2
            \underbrace{\norm{\mathrm{sign}(\xstar_S)}_2}_{= \sqrt{k}}
            \label{eq:delta_ineq}
        \end{equation}

        \item Chain of (in)equalities:
        \begin{align*}
            \norm{\Delta}_1 &= \norm{\Delta_S}_1 + \norm{\Delta_{S^c}}_1
            \leq \sqrt{k} \norm{\Delta_S}_2 +
            \underbrace{\sqrt{k} \norm{\Delta_{S}}_2}_{
                \text{from } \eqref{eq:delta_ineq}}
            \leq 2 \sqrt{k} \norm{\Delta}_2
        \end{align*}
        \item $\norm{A \Delta}_2 = 1 \Rightarrow
        \norm{A \Delta}_2 \geq \norm{\Delta}_2 \sqrt{\gamma_{k}^- (A)}$
    \end{itemize}
\end{frame}
\begin{frame}{Calculating $\mathbb{W}(A \cK)$ - 2}
    Combine all of the above to obtain
    \begin{align*}
        \expec{\abs{\ip{A \Delta, g}}} &= \expec{\abs{\ip{\Delta, A^\top g}}}
        \overset{(\text{H{\"o}lder})}{\leq}
        \expec{\norm{\Delta}_1
        \infnorm{A^\top g}} \\
        & \leq \frac{2 \sqrt{k} \expec{\infnorm{A^\top g}}}{
        \sqrt{\gamma_k^-(A)}} =
        \frac{2 \sqrt{k}}{\sqrt{\gamma_k^-(A)}}
        \expec{\max_{i \in [d]} \abs{a_i^\top g}}
    \end{align*}
    Observe: $a_i^\top g \sim \cN(0, \norm{a_i}_2^2)$ so by a standard argument:
    \[
        \mathbb{W}(A \cK) \leq 6 \sqrt{\frac{k \log d}{\gamma_k^-(A)}}
        \max_{i \in [d]} \norm{a_i}_2
    \]
\end{frame}

\begin{frame}{Calculating $\mathbb{W}(A \cK)$ - 3}
    Finally: $A \cK \subseteq \mathrm{Im}(A)$, so
    \[
        \mathbb{W}(A\cK) \leq \min\set{\rank(A),
        2 \sqrt{\frac{k \log d}{\gamma_k^-(A)}} \max_{i \in [d]} \norm{a_i}_2}
    \]
    \begin{block}{Corollay 3a in~\cite{PilWain15}}
        Consider a sub-gaussian sketch applied to the $\ell_1$-constrained
        least squares. The solution is $\delta$-optimal for
        \[
            m \geq \frac{c_0}{\delta^2} \min\set{\rank(A),
            6 \sqrt{\frac{k \log d}{\gamma_k^-(A)}} \max_{i \in [d]}
            \norm{a_i}_2}.
        \]
    \end{block}
\end{frame}

\bibliography{references}

\end{document}
